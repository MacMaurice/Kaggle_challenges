{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spaceship Titanic Challenge","metadata":{"id":"Ck00s7mTmnjA"}},{"cell_type":"markdown","source":"# Import the library\n\nIn this challenge i will use mainly the tools from fastai, pytorch and sklearn","metadata":{"id":"UPNzfVOEmnjH"}},{"cell_type":"code","source":"from fastai.imports import *\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom fastai.tabular.all import *\n\nnp.set_printoptions(linewidth=130)","metadata":{"id":"mmwBzpblmnjH","execution":{"iopub.status.busy":"2023-09-28T12:40:33.593257Z","iopub.execute_input":"2023-09-28T12:40:33.593739Z","iopub.status.idle":"2023-09-28T12:40:37.947787Z","shell.execute_reply.started":"2023-09-28T12:40:33.593697Z","shell.execute_reply":"2023-09-28T12:40:37.946308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset\n\nI load the 2 datasets (traning and testing set), useful to train my model and to evaluate it","metadata":{"id":"6sHFpppPmnjJ"}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntestset = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\nprint(\"Le dataset d'entrainement est de dimension : {}\".format(dataset_df.shape))","metadata":{"id":"c1P3Y3a7mnjL","execution":{"iopub.status.busy":"2023-09-28T12:40:37.950892Z","iopub.execute_input":"2023-09-28T12:40:37.951719Z","iopub.status.idle":"2023-09-28T12:40:38.081605Z","shell.execute_reply.started":"2023-09-28T12:40:37.951661Z","shell.execute_reply":"2023-09-28T12:40:38.080093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is composed of 14 columns and 8693 entries. We can see all 14 dimensions of our dataset by printing out the first 5 entries using the following code:\n\nMoreover, we can also use df.info() and df.describe() to obtain others valuable informations on the format/values of each column.","metadata":{"id":"cEd92zhJmnjL"}},{"cell_type":"code","source":"dataset_df.head(5)","metadata":{"id":"nCx3PE1xmnjM","execution":{"iopub.status.busy":"2023-09-28T12:40:38.085728Z","iopub.execute_input":"2023-09-28T12:40:38.086137Z","iopub.status.idle":"2023-09-28T12:40:38.134952Z","shell.execute_reply.started":"2023-09-28T12:40:38.086100Z","shell.execute_reply":"2023-09-28T12:40:38.133884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 12 feature columns (14 - `PassengerId` and `Transported` columns). Using these features your model has to predict whether the passenger is rescued or not indicated by the column `Transported`.","metadata":{"id":"0-Euaq6dmnjN"}},{"cell_type":"markdown","source":"# Prepare the dataset","metadata":{"id":"Trlsxv1emnjQ"}},{"cell_type":"markdown","source":"I created the process() function for multiples preprocess steps that we need to make :\nI will split the `Cabin` column into 3 different columns `Deck`, `Cabin_num`,`Side`\nIf it's a dataframe with the `Transported` column (ex : training set), i will remove it and put it into another dataframe.\nI will drop both `PassengerId`,`Cabin` and `Name` columns as they are not necessary for model training.\nI will replace each NaN value by the median for numerical columns, and by the mode (most commun value) for the categorical columns.\nFinally, i will replace every categorical text by values, for example True/False by 1/0","metadata":{"id":"c-BLN1jcmnjQ"}},{"cell_type":"code","source":"def process(df):\n    df1 = df.copy()\n    df1[[\"Deck\", \"Cabin_num\", \"Side\"]] = df1[\"Cabin\"].str.split(\"/\", expand=True)\n    if \"Transported\" in df1.columns :\n        df2 = df1[[\"Transported\"]]\n        df1 = df1.drop(\"Transported\", axis=1)\n        df2 = df2[\"Transported\"].astype(int)\n    else :\n        df2 = None\n    df1 = df1.drop([\"PassengerId\", \"Name\",\"Cabin\"], axis=1)\n    numeric_cols = df1.select_dtypes(include = ['float64', 'int64'])\n    categorical_cols = df1.select_dtypes(include = ['object', 'bool'])\n    médiane = numeric_cols.median()\n    modes = categorical_cols.mode().iloc[0]\n    df1[numeric_cols.columns] = df1[numeric_cols.columns].fillna(médiane)\n    df1[categorical_cols.columns] = df1[categorical_cols.columns].fillna(modes)\n    types_dict = { 'Cabin_num': int,'CryoSleep': int, 'VIP': int}\n    df1 = df1.astype(types_dict)\n    categorical_cols = df1.select_dtypes(include = ['object', 'bool'])\n    for col in categorical_cols.columns:\n        df1[col] = pd.Categorical(df1[col])\n        df1[col] = df1[col].cat.codes\n    return df1,df2\n\nDF, Y = process(dataset_df)\nDF_tst = process(testset)[0]\nDF.head()","metadata":{"id":"nftfBG67mnjR","execution":{"iopub.status.busy":"2023-09-28T12:40:38.137528Z","iopub.execute_input":"2023-09-28T12:40:38.138454Z","iopub.status.idle":"2023-09-28T12:40:38.324510Z","shell.execute_reply.started":"2023-09-28T12:40:38.138410Z","shell.execute_reply":"2023-09-28T12:40:38.323125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the train_test_split() function to separate the training set into a traing set and a cross validation set. I use a seed to ensure the reproductibility of the random parts.","metadata":{}},{"cell_type":"code","source":"seed = 42\ntrn_df, val_df, trn_df_y, val_df_y = train_test_split(DF, Y, test_size=0.2, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:38.325913Z","iopub.execute_input":"2023-09-28T12:40:38.326325Z","iopub.status.idle":"2023-09-28T12:40:38.337014Z","shell.execute_reply.started":"2023-09-28T12:40:38.326277Z","shell.execute_reply":"2023-09-28T12:40:38.335474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function entrainement() fit the model for an input dataframe (ex : training set), and returns the accuracy for the training set and the accuracy for the cross validation set :","metadata":{}},{"cell_type":"code","source":"rf0 = RandomForestClassifier(500, min_samples_leaf = 15)\n\ndef numper(df) :\n    return np.array(df).ravel()\n\ndef precision (modele, reel) :\n    return ((modele == reel).sum())/len(reel)\n\ndef entrainement (df,df_y,df_val,df_val_y,rf):\n    rf.fit(df, numper(df_y))\n    return precision(rf.predict(df),df_y),precision(rf.predict(df_val),df_val_y)\n\nentrainement (trn_df,trn_df_y,val_df,val_df_y,rf0) ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:38.339684Z","iopub.execute_input":"2023-09-28T12:40:38.340693Z","iopub.status.idle":"2023-09-28T12:40:43.303373Z","shell.execute_reply.started":"2023-09-28T12:40:38.340622Z","shell.execute_reply":"2023-09-28T12:40:43.301824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"evolution() function is just a function that i used to see how can i optimize the parameters of the RandomForestClassifier function, i.e the number of trees and the minimum of leafs.","metadata":{}},{"cell_type":"code","source":"def evolution(df,df_y,df_val,df_val_y):\n    Precision_trn1 =[]\n    Precision_val1 =[]\n    Precision_trn2 =[]\n    Precision_val2 =[]\n    Abcisses1 = []\n    Abcisses2 = []\n    for i in range(10,511,50):\n        rf = RandomForestClassifier(i, min_samples_leaf = 5)\n        rf.fit(df, numper(df_y))\n        Abcisses1.append(i)\n        Precision_trn1.append(precision(rf.predict(df),df_y))\n        Precision_val1.append(precision(rf.predict(df_val),df_val_y))\n    for j in range(2,102,5):\n        rf = RandomForestClassifier(300, min_samples_leaf = j)\n        rf.fit(df, numper(df_y))\n        Abcisses2.append(j)\n        Precision_trn2.append(precision(rf.predict(df),df_y))\n        Precision_val2.append(precision(rf.predict(df_val),df_val_y))\n    plt.subplot(1, 2, 1)\n    plt.plot(Abcisses1, Precision_val1, label = 'Precision Cross Validation Set', marker = 'x')\n    plt.xlabel(\"Nombre d'Arbres de la Random Forest\")\n    plt.ylabel('Taux de Précision')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(Abcisses2, Precision_val2, label = 'Precision Cross Validation Set', marker = 'x')\n    plt.xlabel(\"Taille de Ramification Minimale\")\n    plt.ylabel('Taux de Précision')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n        \n#evolution (trn_df,trn_df_y,val_df,val_df_y) ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.305323Z","iopub.execute_input":"2023-09-28T12:40:43.305791Z","iopub.status.idle":"2023-09-28T12:40:43.323378Z","shell.execute_reply.started":"2023-09-28T12:40:43.305751Z","shell.execute_reply":"2023-09-28T12:40:43.321648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.325029Z","iopub.execute_input":"2023-09-28T12:40:43.325552Z","iopub.status.idle":"2023-09-28T12:40:43.359725Z","shell.execute_reply.started":"2023-09-28T12:40:43.325496Z","shell.execute_reply":"2023-09-28T12:40:43.358205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"subm() function is to create the output csv for a model and a testing dataframe","metadata":{}},{"cell_type":"code","source":"def subm(preds, suff):\n    testset[\"Transported\"] = preds\n    testset[\"Transported\"] = testset[\"Transported\"].astype(bool)\n    sub_df = testset[[\"PassengerId\",\"Transported\"]]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\n#subm(rf0.predict(DF_tst), 'Random_Forest')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.362184Z","iopub.execute_input":"2023-09-28T12:40:43.363297Z","iopub.status.idle":"2023-09-28T12:40:43.371256Z","shell.execute_reply.started":"2023-09-28T12:40:43.363219Z","shell.execute_reply":"2023-09-28T12:40:43.370281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train a Neural Network, i used a second step of preprocessing. Unlike the random forest algorithm, it's useful for a NN to normalize the distribution of the columns. To do that, i used the logarithm of some numericals columns :","metadata":{}},{"cell_type":"code","source":"def process_nn(df,df_y = None):\n    df_nn = df.copy()\n    if df_y is not None :\n        df_nn[\"Transported\"] = df_y\n    df_nn['LogShoppingMall'] = np.log1p(df_nn['ShoppingMall'])\n    df_nn['LogFoodCourt'] = np.log1p(df_nn['FoodCourt'])\n    df_nn['LogSpa'] = np.log1p(df_nn['Spa'])\n    df_nn['LogVRDeck'] = np.log1p(df_nn['VRDeck'])\n    df_nn = df_nn.drop([\"ShoppingMall\", \"FoodCourt\",\"Spa\",\"VRDeck\"], axis=1)\n    return df_nn\n    \nDF_NN = process_nn(DF,Y)\nDF_tst_NN = process_nn(DF_tst)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.376645Z","iopub.execute_input":"2023-09-28T12:40:43.377569Z","iopub.status.idle":"2023-09-28T12:40:43.403587Z","shell.execute_reply.started":"2023-09-28T12:40:43.377510Z","shell.execute_reply":"2023-09-28T12:40:43.402480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use the TabularPandas() function to make the things clean by itself :","metadata":{}},{"cell_type":"code","source":"splits = RandomSplitter(seed=42)(DF_NN)\nDF_NN.head()\n\ndls = TabularPandas(\n    DF_NN, splits=splits,\n    procs = [Categorify, FillMissing, Normalize],\n    cat_names=[\"HomePlanet\",\"CryoSleep\",\"Destination\",\"VIP\", \"Deck\",\"Cabin_num\",\"Side\",\"RoomService\"],\n    cont_names=['Age', 'LogFoodCourt', 'LogShoppingMall', 'LogSpa', 'LogVRDeck'],\n    y_names=\"Transported\", y_block = CategoryBlock(),\n).dataloaders(path=\".\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.405334Z","iopub.execute_input":"2023-09-28T12:40:43.406138Z","iopub.status.idle":"2023-09-28T12:40:43.567198Z","shell.execute_reply.started":"2023-09-28T12:40:43.406085Z","shell.execute_reply":"2023-09-28T12:40:43.565817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, we have to create the initial model, using the tabular_learner() function, the metric and the layers that i want :","metadata":{}},{"cell_type":"code","source":"learn = tabular_learner(dls, metrics=accuracy, layers=[3,3])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.571470Z","iopub.execute_input":"2023-09-28T12:40:43.571898Z","iopub.status.idle":"2023-09-28T12:40:43.633795Z","shell.execute_reply.started":"2023-09-28T12:40:43.571861Z","shell.execute_reply":"2023-09-28T12:40:43.632341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lr_find() is a informative function to have a sense of what could be a good learning rate :","metadata":{}},{"cell_type":"code","source":"Sugest_lr = learn.lr_find(suggest_funcs=(slide, valley))\nSugest_lr","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:43.635810Z","iopub.execute_input":"2023-09-28T12:40:43.636720Z","iopub.status.idle":"2023-09-28T12:40:46.436355Z","shell.execute_reply.started":"2023-09-28T12:40:43.636662Z","shell.execute_reply":"2023-09-28T12:40:46.434952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can fit the model with the data considering a number of iterations (epoch) and a learning rate that we can define using the graphic.","metadata":{}},{"cell_type":"code","source":"learn.fit(2, lr = 0.04)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:46.438794Z","iopub.execute_input":"2023-09-28T12:40:46.439350Z","iopub.status.idle":"2023-09-28T12:40:50.234086Z","shell.execute_reply.started":"2023-09-28T12:40:46.439290Z","shell.execute_reply":"2023-09-28T12:40:50.232729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The argument test_dl() is to use the same parameters than the previous set for the test set.\nThat done, we can make our predictions :","metadata":{}},{"cell_type":"code","source":"tst_dl = learn.dls.test_dl(DF_tst_NN)\nprediction,_ = learn.get_preds(dl = tst_dl)\nprediction = (prediction[:,1]>0.5).int()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:50.235877Z","iopub.execute_input":"2023-09-28T12:40:50.236737Z","iopub.status.idle":"2023-09-28T12:40:50.696843Z","shell.execute_reply.started":"2023-09-28T12:40:50.236678Z","shell.execute_reply":"2023-09-28T12:40:50.695391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm(prediction, 'NN')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:50.699381Z","iopub.execute_input":"2023-09-28T12:40:50.700220Z","iopub.status.idle":"2023-09-28T12:40:50.730983Z","shell.execute_reply.started":"2023-09-28T12:40:50.700162Z","shell.execute_reply":"2023-09-28T12:40:50.729525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}